Model,Overall Score,Accuracy,Relevance,Coherence,Factuality,Completeness,Response Time,Pass Rate
Claude 3 Opus,73.88,66.48,78.11,98.33,50.76,89.43,2667,73.33
Mistral Large,73.44,66.24,75.22,98.33,52.26,88.81,2762,73.33
Vishnu AI (Qwen 2.5 72B),73.38,66.03,75.67,98.33,50.96,89.94,2468,73.33
Command R+,70.86,63.48,72.56,96.67,50.35,84.92,2549,73.33
DeepSeek V3,70.48,62.22,76.78,96.67,45.90,85.17,2530,66.67
Gemini 2.0 Flash,69.32,59.91,74.00,96.67,41.36,91.81,3004,66.67
Llama 3.3 70B,64.35,55.06,64.56,95.00,41.51,82.48,2450,60.00
Gemini 1.5 Pro,64.30,55.79,65.44,93.33,42.32,80.10,2423,60.00
GPT-4o Mini,63.14,55.26,62.78,93.33,44.86,73.56,2093,60.00
Claude 3.5 Sonnet,61.40,52.89,59.22,93.33,42.96,74.00,2197,53.33
GPT-4 Turbo,51.99,41.35,50.89,90.00,30.07,65.94,2025,46.67